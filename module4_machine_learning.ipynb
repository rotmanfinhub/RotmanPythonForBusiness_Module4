{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 4: Machine Learning\n",
    "\n",
    "**Python for Business | FinHub**\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "![Module 4 Preview](images/module4_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "by-the-end",
   "metadata": {},
   "source": [
    "## By the End of This Module\n",
    "\n",
    "You'll be able to:\n",
    "\n",
    "| Skill | Example |\n",
    "|-------|--------|\n",
    "| Discover groups in data | \"These 20 stocks cluster into 2 natural groups\" |\n",
    "| Predict categories | \"Will the stock go UP or DOWN tomorrow?\" |\n",
    "| Interpret feature importance | \"Volume change matters more than today's return\" |\n",
    "| Evaluate out-of-sample | Train on 4 years, test on 1 year of unseen data |\n",
    "\n",
    "**Two techniques:**\n",
    "1. **K-Means Clustering** — Find groups without labels (unsupervised)\n",
    "2. **Random Forest Classification** — Predict categories and see which features matter (supervised)\n",
    "\n",
    "Both use the same scikit-learn patterns you learned in Module 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load stock characteristics\n",
    "stock_df = pd.read_csv(\"data/stock_characteristics.csv\")\n",
    "print(f\"Loaded {len(stock_df)} stocks\")\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clustering-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. K-Means Clustering\n",
    "\n",
    "### The Question\n",
    "\n",
    "Imagine you're an analyst looking at 20 stocks. You have two characteristics for each:\n",
    "- **Beta**: How volatile is it relative to the market?\n",
    "- **Dividend Yield**: What percentage does it pay out?\n",
    "\n",
    "**Question: Can we find meaningful investment groups based only on these numbers?**\n",
    "\n",
    "We don't have labels. We just have data. This is **unsupervised learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we have?\n",
    "print(\"Our 20 stocks:\")\n",
    "print(stock_df[[\"ticker\", \"beta\", \"dividend_yield\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-unlabeled",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: just the data, no labels\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "ax.scatter(stock_df[\"beta\"], stock_df[\"dividend_yield\"], s=150, alpha=0.7)\n",
    "\n",
    "for _, row in stock_df.iterrows():                    # Label each point\n",
    "    ax.annotate(row[\"ticker\"], (row[\"beta\"], row[\"dividend_yield\"]),\n",
    "                xytext=(5, 5), textcoords=\"offset points\", fontsize=9)\n",
    "\n",
    "ax.set_xlabel(\"Beta (Market Sensitivity)\", fontsize=12)\n",
    "ax.set_ylabel(\"Dividend Yield (%)\", fontsize=12)\n",
    "ax.set_title(\"20 Stocks: Can You See Two Groups?\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot. Do you see a pattern?\")\n",
    "print(\"Some stocks cluster in the lower-left, others in the upper-right...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans-explain",
   "metadata": {},
   "source": [
    "### K-Means Algorithm\n",
    "\n",
    "K-Means finds groups by:\n",
    "1. Pick K random points as initial \"centers\"\n",
    "2. Assign each stock to its nearest center\n",
    "3. Recalculate centers as the average of each group\n",
    "4. Repeat until stable\n",
    "\n",
    "**Important:** Always standardize features first. K-Means uses distance, so features on different scales would dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standardize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (mean=0, std=1)\n",
    "features = [\"beta\", \"dividend_yield\"]\n",
    "X = stock_df[features].values                         # Raw data\n",
    "\n",
    "scaler = StandardScaler()                             # Create scaler\n",
    "X_scaled = scaler.fit_transform(X)                    # Fit and transform\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(f\"  Beta range: {X[:, 0].min():.2f} to {X[:, 0].max():.2f}\")\n",
    "print(f\"  Dividend range: {X[:, 1].min():.2f} to {X[:, 1].max():.2f}\")\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(f\"  Beta range: {X_scaled[:, 0].min():.2f} to {X_scaled[:, 0].max():.2f}\")\n",
    "print(f\"  Dividend range: {X_scaled[:, 1].min():.2f} to {X_scaled[:, 1].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-kmeans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with K=2 clusters\n",
    "kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "kmeans.fit(X_scaled)                                  # Fit on standardized data\n",
    "\n",
    "# Add cluster labels to our data\n",
    "stock_df[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "print(\"Cluster assignments:\")\n",
    "print(stock_df[[\"ticker\", \"beta\", \"dividend_yield\", \"cluster\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters K-Means found\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "colors = [\"#2ecc71\", \"#9b59b6\"]                       # Green and purple\n",
    "for cluster in [0, 1]:\n",
    "    mask = stock_df[\"cluster\"] == cluster\n",
    "    ax.scatter(\n",
    "        stock_df.loc[mask, \"beta\"],\n",
    "        stock_df.loc[mask, \"dividend_yield\"],\n",
    "        c=colors[cluster], s=150, label=f\"Cluster {cluster}\", alpha=0.7\n",
    "    )\n",
    "\n",
    "for _, row in stock_df.iterrows():\n",
    "    ax.annotate(row[\"ticker\"], (row[\"beta\"], row[\"dividend_yield\"]),\n",
    "                xytext=(5, 5), textcoords=\"offset points\", fontsize=9)\n",
    "\n",
    "ax.set_xlabel(\"Beta\", fontsize=12)\n",
    "ax.set_ylabel(\"Dividend Yield (%)\", fontsize=12)\n",
    "ax.set_title(\"K-Means Found Two Groups (No Labels Used!)\", fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reveal-truth",
   "metadata": {},
   "source": [
    "### The Reveal: What Did K-Means Find?\n",
    "\n",
    "Let's check what these clusters actually represent. Our data has a \"sector\" column we haven't looked at yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-sectors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What sectors are in each cluster?\n",
    "print(\"Cluster composition:\")\n",
    "for cluster in [0, 1]:\n",
    "    cluster_stocks = stock_df[stock_df[\"cluster\"] == cluster]\n",
    "    sectors = cluster_stocks[\"sector\"].value_counts()\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    for sector, count in sectors.items():\n",
    "        print(f\"  {sector}: {count} stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side by side: K-Means clusters vs True sectors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: K-Means clusters\n",
    "for cluster in [0, 1]:\n",
    "    mask = stock_df[\"cluster\"] == cluster\n",
    "    axes[0].scatter(\n",
    "        stock_df.loc[mask, \"beta\"],\n",
    "        stock_df.loc[mask, \"dividend_yield\"],\n",
    "        c=colors[cluster], s=150, label=f\"Cluster {cluster}\", alpha=0.7\n",
    "    )\n",
    "for _, row in stock_df.iterrows():\n",
    "    axes[0].annotate(row[\"ticker\"], (row[\"beta\"], row[\"dividend_yield\"]),\n",
    "                     xytext=(5, 5), textcoords=\"offset points\", fontsize=9)\n",
    "axes[0].set_xlabel(\"Beta\")\n",
    "axes[0].set_ylabel(\"Dividend Yield (%)\")\n",
    "axes[0].set_title(\"K-Means Clusters (Unsupervised)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: True sectors\n",
    "sector_colors = {\"Utilities\": \"#3498db\", \"Tech\": \"#e74c3c\"}\n",
    "for sector, color in sector_colors.items():\n",
    "    mask = stock_df[\"sector\"] == sector\n",
    "    axes[1].scatter(\n",
    "        stock_df.loc[mask, \"beta\"],\n",
    "        stock_df.loc[mask, \"dividend_yield\"],\n",
    "        c=color, s=150, label=sector, alpha=0.7\n",
    "    )\n",
    "for _, row in stock_df.iterrows():\n",
    "    axes[1].annotate(row[\"ticker\"], (row[\"beta\"], row[\"dividend_yield\"]),\n",
    "                     xytext=(5, 5), textcoords=\"offset points\", fontsize=9)\n",
    "axes[1].set_xlabel(\"Beta\")\n",
    "axes[1].set_ylabel(\"Dividend Yield (%)\")\n",
    "axes[1].set_title(\"True Sector Labels\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"K-Means recovered the sector groupings almost perfectly — with no labels!\")\n",
    "print(\"It found that Utilities (low beta, high dividend) cluster separately from Tech (high beta, low dividend).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-1",
   "metadata": {},
   "source": [
    "### ✏️ Checkpoint: Try K=3 Clusters\n",
    "\n",
    "What happens if we ask for 3 clusters instead of 2? Does it find a meaningful third group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checkpoint-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yiwskenm5qe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Random Forest Classification\n",
    "\n",
    "### A Different Question\n",
    "\n",
    "In Module 3, we tried to predict the *exact* return. That's hard — R² is basically zero.\n",
    "\n",
    "But what if we ask an easier question: **Will the stock go UP or DOWN tomorrow?**\n",
    "\n",
    "This is **classification** instead of regression. Instead of predicting a number, we predict a category.\n",
    "\n",
    "And instead of R², we measure **accuracy**: what percentage of days did we get right? If we're just guessing, we'd expect 50%. Can we beat that?\n",
    "\n",
    "We'll use features from our dataset:\n",
    "- **Basic**: today's return, volume change, price range, overnight gap\n",
    "- **Technical**: 20-day volatility, 5-day momentum\n",
    "- **Market**: S&P 500 return, VIX level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wyrfjsaubke",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load and prepare stock data with features\n",
    "df = pd.read_csv(\"data/stock_prices.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Calculate returns within each ticker\n",
    "df[\"return\"] = df.groupby(\"ticker\")[\"close\"].pct_change()\n",
    "\n",
    "# Feature engineering (computed within each ticker group)\n",
    "df[\"volume_chg\"] = df.groupby(\"ticker\")[\"volume\"].pct_change()\n",
    "df[\"range_pct\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"]\n",
    "df[\"gap\"] = (df[\"open\"] - df.groupby(\"ticker\")[\"close\"].shift(1)) / df.groupby(\"ticker\")[\"close\"].shift(1)\n",
    "\n",
    "# Moving averages and technical features\n",
    "df[\"ma5\"] = df.groupby(\"ticker\")[\"close\"].transform(lambda x: x.rolling(5).mean())\n",
    "df[\"ma20\"] = df.groupby(\"ticker\")[\"close\"].transform(lambda x: x.rolling(20).mean())\n",
    "df[\"price_vs_ma5\"] = (df[\"close\"] - df[\"ma5\"]) / df[\"ma5\"]\n",
    "df[\"price_vs_ma20\"] = (df[\"close\"] - df[\"ma20\"]) / df[\"ma20\"]\n",
    "\n",
    "# Volatility and momentum\n",
    "df[\"volatility_5d\"] = df.groupby(\"ticker\")[\"return\"].transform(lambda x: x.rolling(5).std())\n",
    "df[\"volatility_20d\"] = df.groupby(\"ticker\")[\"return\"].transform(lambda x: x.rolling(20).std())\n",
    "df[\"momentum_5d\"] = df.groupby(\"ticker\")[\"close\"].transform(lambda x: x.pct_change(5))\n",
    "df[\"momentum_20d\"] = df.groupby(\"ticker\")[\"close\"].transform(lambda x: x.pct_change(20))\n",
    "\n",
    "# Load market data and add SPY return and VIX\n",
    "market_df = pd.read_csv(\"data/market_data.csv\")\n",
    "market_df[\"date\"] = pd.to_datetime(market_df[\"date\"])\n",
    "\n",
    "spy = market_df[market_df[\"ticker\"] == \"SPY\"][[\"date\", \"close_px\"]].rename(columns={\"close_px\": \"spy_close\"})\n",
    "spy[\"spy_return\"] = spy[\"spy_close\"].pct_change()\n",
    "vix = market_df[market_df[\"ticker\"] == \"VIX\"][[\"date\", \"close_px\"]].rename(columns={\"close_px\": \"vix\"})\n",
    "\n",
    "df = df.merge(spy[[\"date\", \"spy_return\"]], on=\"date\", how=\"left\")\n",
    "df = df.merge(vix[[\"date\", \"vix\"]], on=\"date\", how=\"left\")\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "print(f\"Tickers: {df['ticker'].unique().tolist()}\")\n",
    "\n",
    "# Show available features\n",
    "print(f\"\\nFeatures available:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duvmpee1whd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on AAPL\n",
    "aapl = df[df[\"ticker\"] == \"AAPL\"].copy().set_index(\"date\")\n",
    "\n",
    "# Target: will tomorrow go UP (1) or DOWN (0)?\n",
    "aapl[\"tomorrow_return\"] = aapl[\"return\"].shift(-1)\n",
    "aapl[\"direction\"] = (aapl[\"tomorrow_return\"] > 0).astype(int)\n",
    "\n",
    "# Features we'll use\n",
    "features = [\n",
    "    # Basic (things we observe today)\n",
    "    \"return\", \"volume_chg\", \"range_pct\", \"gap\",\n",
    "    # Technical\n",
    "    \"volatility_20d\", \"momentum_5d\",\n",
    "    # Market\n",
    "    \"spy_return\", \"vix\",\n",
    "]\n",
    "\n",
    "# Prepare data (drop rows with missing values)\n",
    "class_data = aapl[features + [\"direction\"]].dropna()\n",
    "split_idx = int(len(class_data) * 0.8)\n",
    "\n",
    "train = class_data.iloc[:split_idx]\n",
    "test = class_data.iloc[split_idx:]\n",
    "\n",
    "X_train = train[features].values\n",
    "y_train = train[\"direction\"].values\n",
    "X_test = test[features].values\n",
    "y_test = test[\"direction\"].values\n",
    "\n",
    "print(f\"Using {len(features)} features:\")\n",
    "for f in features:\n",
    "    print(f\"  • {f}\")\n",
    "print(f\"\\nTraining: {len(train)} days\")\n",
    "print(f\"Testing:  {len(test)} days\")\n",
    "print(f\"\\nClass balance in test data:\")\n",
    "print(f\"  Up days:   {y_test.sum()} ({y_test.mean():.1%})\")\n",
    "print(f\"  Down days: {len(y_test) - y_test.sum()} ({1 - y_test.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z7fhyxu84e",
   "metadata": {},
   "source": [
    "### What is Random Forest?\n",
    "\n",
    "A Random Forest is a collection of decision trees. Each tree learns simple rules like:\n",
    "- \"If volatility > 0.02 AND momentum < 0, predict DOWN\"\n",
    "- \"If volume is high AND price is above MA, predict UP\"\n",
    "\n",
    "The \"forest\" part: instead of one tree, we build many trees (100 by default), each trained on a random subset of the data and features. The final prediction is a **vote** — whatever most trees predict wins.\n",
    "\n",
    "Why this works:\n",
    "- Individual trees might overfit, but averaging many trees reduces noise\n",
    "- Different trees focus on different patterns\n",
    "- It's robust and hard to break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i4ou7oh4o2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=250, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_test = rf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RANDOM FOREST: Predicting UP or DOWN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training accuracy: {acc_train:.1%}\")\n",
    "print(f\"Test accuracy:     {acc_test:.1%}\")\n",
    "print(f\"\")\n",
    "print(f\"Baseline (random guessing): 50%\")\n",
    "print(f\"Improvement over baseline:  {acc_test - 0.5:+.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l7r4wgc4yzf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix: where did we get it right/wrong?\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels([\"Predicted DOWN\", \"Predicted UP\"])\n",
    "ax.set_yticklabels([\"Actual DOWN\", \"Actual UP\"])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=20,\n",
    "                color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "\n",
    "ax.set_title(\"Confusion Matrix (Test Set)\")\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Reading the matrix:\")\n",
    "print(f\"  Correctly predicted DOWN: {cm[0,0]} days\")\n",
    "print(f\"  Correctly predicted UP:   {cm[1,1]} days\")\n",
    "print(f\"  Wrong (predicted UP, was DOWN):   {cm[0,1]} days\")\n",
    "print(f\"  Wrong (predicted DOWN, was UP):   {cm[1,0]} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ivv4v8q1pw",
   "metadata": {},
   "source": [
    "### Feature Importance: What Does the Model Care About?\n",
    "\n",
    "One of the best things about Random Forest: it tells you **which features matter most**.\n",
    "\n",
    "The importance score measures how much each feature helps the trees make better splits. Higher = more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hkfbot210nj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"importance\": rf.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = plt.cm.Blues(importance_df[\"importance\"] / importance_df[\"importance\"].max())\n",
    "ax.barh(importance_df[\"feature\"], importance_df[\"importance\"], color=colors)\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_title(\"Random Forest: Which Features Matter Most?\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top features\n",
    "print(\"Most important features:\")\n",
    "for _, row in importance_df.tail(5).iloc[::-1].iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tg7fpmw1btd",
   "metadata": {},
   "source": [
    "### The Takeaway\n",
    "\n",
    "We achieved a small improvement over random guessing (50%).\n",
    "\n",
    "That might sound disappointing, but consider:\n",
    "- **Stock returns are hard to predict.** This is a fundamental result in finance.\n",
    "- **Even small edges matter.** A few percent edge, applied consistently over thousands of trades, can be meaningful.\n",
    "- **The model learned something real.** Look at the feature importance — it's telling us which signals have predictive power.\n",
    "\n",
    "What's interesting about the feature importance:\n",
    "- **Momentum features** often rank highly — recent trends have some predictive power\n",
    "- **Volatility** measures how \"noisy\" the stock is\n",
    "- **Volume changes** can signal unusual activity\n",
    "\n",
    "The honest truth: you're not going to get rich from a simple model like this. But you now understand how practitioners approach prediction problems in finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85br26ri42g",
   "metadata": {},
   "source": [
    "### ✏️ Checkpoint: Tune the Forest\n",
    "\n",
    "Try changing these Random Forest parameters and see how accuracy changes:\n",
    "- `n_estimators`: Number of trees (default 100). More trees = more stable but slower.\n",
    "- `max_depth`: How deep each tree can go (default None = unlimited). Shallower = less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dyh9pjy5l5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exercises\n",
    "\n",
    "Complete these to finish Module 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Elbow Method\n",
    "\n",
    "How do we choose K for K-Means? One way: plot the \"inertia\" (within-cluster sum of squares) for K=1,2,3,...,6. Look for an \"elbow\" where adding more clusters stops helping much.\n",
    "\n",
    "```python\n",
    "inertias = []\n",
    "for k in range(1, 7):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "plt.plot(range(1, 7), inertias, marker='o')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Try a Different Stock\n",
    "\n",
    "Does Random Forest work better or worse on different stocks? Try running the classification on NVDA or MSFT instead of AAPL.\n",
    "\n",
    "```python\n",
    "# Change the ticker and re-run the classification\n",
    "ticker = \"NVDA\"  # or \"MSFT\", \"GOOGL\", etc.\n",
    "stock = df[df[\"ticker\"] == ticker].copy().set_index(\"date\")\n",
    "# ... (same feature engineering as before)\n",
    "```\n",
    "\n",
    "Does the accuracy change? Does the feature importance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here — try a different stock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Commit Your Work\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Complete module 4 exercises\"\n",
    "git push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recap\n",
    "\n",
    "| Technique | Use When | Key Method |\n",
    "|-----------|----------|------------|\n",
    "| **K-Means** | Find groups without labels | `KMeans(n_clusters=K).fit(X)` |\n",
    "| **Random Forest** | Predict categories, understand feature importance | `RandomForestClassifier().fit(X, y)` |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **K-Means discovers structure** — it found industry groupings from just beta and dividend yield\n",
    "2. **Always standardize** before K-Means (it uses distance)\n",
    "3. **Classification reframes the problem** — \"up or down?\" is easier to interpret than \"what's the return?\"\n",
    "4. **Feature importance** shows what the model learned\n",
    "5. **Out-of-sample evaluation is essential** — train accuracy can lie, test accuracy tells the truth\n",
    "6. **Small edges matter in finance** — even a few percent above 50% is meaningful at scale\n",
    "\n",
    "---\n",
    "\n",
    "**Next up:** Module 5 — Responsible Coding with AI. We'll build a project from scratch using AI assistance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonforbusiness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
